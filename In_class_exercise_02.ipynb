{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "In-class-exercise-02(1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharath-11507563/sharath_INFO5731_-Fall2021/blob/main/In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woFcXwDRgrIi"
      },
      "source": [
        "## The third In-class-exercise (9/15/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nONJYKrFgrIt"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_PBuPXegrIw"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHELwe85grIy",
        "outputId": "d4710012-14e5-43e2-cfc6-8abab4fcb36a"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "Please write you answer here:\n",
        "Research question- Identifying the sentiment behind a text paragraph. For example, reading a review about a product\n",
        "and categorising ito into different buckets such as extremely positive,mildy positive,neutral,midly negative and \n",
        "extremely negative. This will help us categorise the reviews received on a newly launched product to identify \n",
        "keywords from the reviews. These keywords help us in identifying the positives and negatives of this newly launched\n",
        "product. \n",
        "\n",
        "Data needed-\n",
        "Name of the reviewer, Rating given, Review text, Date of the review\n",
        "\n",
        "Quantity of data-\n",
        "As we are talking about a new product, our sample size would be low until the product becomes popular. We should be \n",
        "expecting a minimum of 50 reviews. \n",
        "\n",
        "Steps-\n",
        "Use beautifulsoup to extract information\n",
        "Create a dataframe to represent the list and save it into a csv file.\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VHKq5kzgrI5"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwL4ctadgrI8",
        "outputId": "91ac6954-5836-453e-b213-55afc3996a06"
      },
      "source": [
        "#importing libraries\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://www.imdb.com/title/tt0468569/reviews\"\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text,'html.parser')\n",
        "#finding list of reviews\n",
        "all_review=soup.find('div',class_=\"lister-list\")\n",
        "#finding each review\n",
        "each_review=all_review.find_all('div',class_=\"lister-item-content\")\n",
        "#calling first review\n",
        "review =each_review[0]\n",
        "print(review.prettify())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<div class=\"lister-item-content\">\n",
            " <div class=\"ipl-ratings-bar\">\n",
            "  <span class=\"rating-other-user-rating\">\n",
            "   <svg class=\"ipl-icon ipl-star-icon\" fill=\"#000000\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "    <path d=\"M0 0h24v24H0z\" fill=\"none\">\n",
            "    </path>\n",
            "    <path d=\"M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z\">\n",
            "    </path>\n",
            "    <path d=\"M0 0h24v24H0z\" fill=\"none\">\n",
            "    </path>\n",
            "   </svg>\n",
            "   <span>\n",
            "    10\n",
            "   </span>\n",
            "   <span class=\"point-scale\">\n",
            "    /10\n",
            "   </span>\n",
            "  </span>\n",
            " </div>\n",
            " <a class=\"title\" href=\"/review/rw6467314/\">\n",
            "  Perfect combo\n",
            " </a>\n",
            " <div class=\"display-name-date\">\n",
            "  <span class=\"display-name-link\">\n",
            "   <a href=\"/user/ur95396995/\">\n",
            "    harrybhangu\n",
            "   </a>\n",
            "  </span>\n",
            "  <span class=\"review-date\">\n",
            "   12 January 2021\n",
            "  </span>\n",
            " </div>\n",
            " <div class=\"content\">\n",
            "  <div class=\"text show-more__control\">\n",
            "   Best movie ever. Heath ledger's work is phenomenal no words......\n",
            "  </div>\n",
            "  <div class=\"actions text-muted\">\n",
            "   141 out of 152 found this helpful.\n",
            "   <span>\n",
            "    Was this review helpful?\n",
            "    <a href=\"/registration/signin\">\n",
            "     Sign in\n",
            "    </a>\n",
            "    to vote.\n",
            "   </span>\n",
            "   <br/>\n",
            "   <a href=\"/review/rw6467314/\">\n",
            "    Permalink\n",
            "   </a>\n",
            "  </div>\n",
            " </div>\n",
            "</div>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kHXaxL-grI_",
        "outputId": "9cafe28e-8f90-4322-a919-80e8e0b182eb"
      },
      "source": [
        "#finding individual attribute list\n",
        "user_name=review.find('span',class_=\"display-name-link\").get_text()\n",
        "star_rating=review.find('span',class_=\"rating-other-user-rating\").get_text()\n",
        "review_title=review.find('a',class_=\"title\").get_text()\n",
        "review_text=review.find('div',class_=\"text show-more__control\").get_text()\n",
        "review_time=review.find('span',class_=\"review-date\").get_text()\n",
        "#printing using formatted string\n",
        "print(f'{user_name.strip()}')\n",
        "print(f'{star_rating.strip()}')\n",
        "print(f'{review_title.strip()}')\n",
        "print(f'{review_text.strip()}')\n",
        "print(f'{review_time.strip()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "harrybhangu\n",
            "10/10\n",
            "Perfect combo\n",
            "Best movie ever. Heath ledger's work is phenomenal no words......\n",
            "12 January 2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WrICfzZgrJC",
        "outputId": "31e46837-24e2-4896-f4a5-7e132aa09000"
      },
      "source": [
        "#creating individual lists\n",
        "username =[un.get_text() for un in all_review.select(\".lister-item-content .display-name-link\")]\n",
        "username"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['harrybhangu',\n",
              " 'elmerivikki',\n",
              " 'Aegontheconqueror',\n",
              " 'dseferaj',\n",
              " 'MR_Heraclius',\n",
              " 'Smells_Like_Cheese',\n",
              " 'straightblaster',\n",
              " 'filmquestint',\n",
              " 'mahmoudr-13451',\n",
              " 'krazycrukids',\n",
              " 'sjvkarki',\n",
              " 'has1er',\n",
              " 'baltazar07',\n",
              " 'littlemartinarocena',\n",
              " 'redbullrex',\n",
              " 'maka12',\n",
              " 'SimonZW',\n",
              " 'johnnymacbest',\n",
              " 'Impartial-Reviewer',\n",
              " 'guerillagorilla',\n",
              " 'jwjwpnvq',\n",
              " 'Mush_Man29',\n",
              " 'LoneWolfAndCub',\n",
              " 'LennyRenquist',\n",
              " 'rustyalex2']"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EWXKLQjgrJI",
        "outputId": "e8088b79-0d3b-4203-d6fa-24be7bfd037a"
      },
      "source": [
        "starrating =[sr.get_text() for sr in all_review.select(\".lister-item-content .rating-other-user-rating\")]\n",
        "reviewtitle =[rt.get_text() for rt in all_review.select(\".lister-item-content .title\")]\n",
        "reviewtext =[rx.get_text() for rx in all_review.select(\".lister-item-content .text show-more__control\")]\n",
        "reviewtime =[rm.get_text() for rm in all_review.select(\".lister-item-content .review-date\")]\n",
        "starrating=[item.strip() for item in starrating]\n",
        "reviewtitle=[item.strip() for item in reviewtitle]\n",
        "print(starrating)\n",
        "print(reviewtitle)\n",
        "print(reviewtext)\n",
        "print(reviewtime)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '9/10', '10/10', '9/10', '9/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10', '10/10']\n",
            "['Perfect combo', 'The Dark Knight', 'MASTERPIECE', 'This town deserves a better class of criminal!', 'The Dark Knight', 'The Batman of our dreams! So much more than a comic book movie', 'Film surpasses the hype', \"Heath Ledger's Dark and Brilliant Swan Song\", 'Best movie ever', 'Fantastic!', \"Best in it's class and much more\", \"9.4 Top Contender for 'Best Superhero Movie'\", 'One of the Best Superhero movies EVER!!!!!!!', 'A Batman Of Shakesperean Proportions', 'Heath Ledger', 'One of the best movies', 'The Dark Knight Movie Review', 'Surpasses \"Begins\" in every aspect!!!', 'The Most Realistic True to Heart Movie Based on Comic Books.', 'The sequel we deserved to the Batman we wanted', 'The Most Re-watchable Movie of All Time', 'Certainly Dark - Worth The Wait', 'The Dark Knight is as good as everyone says and easily the best superhero film made', 'Best. Comic. Movie. EVER.', 'Heath Ledger is a ledgend!']\n",
            "[]\n",
            "['12 January 2021', '9 January 2021', '7 October 2019', '17 February 2021', '12 February 2020', '20 July 2008', '9 July 2008', '20 July 2008', '19 January 2021', '11 January 2012', '16 January 2021', '14 January 2021', '15 January 2012', '24 July 2008', '22 January 2021', '15 January 2021', '6 February 2021', '7 July 2008', '20 February 2020', '16 July 2008', '18 July 2021', '16 July 2008', '16 July 2008', '16 July 2008', '16 July 2008']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "-CETqTRsgrJM",
        "outputId": "320fa0c7-c076-47a6-c23f-5b2c368c9d00"
      },
      "source": [
        "#creating dataframe to print as a table\n",
        "import pandas as pd\n",
        "reviews = pd.DataFrame\n",
        "#df = pd.DataFrame((username,starrating,reviewtitle,reviewtext,reviewtime), columns =['Name', 'Rating','Title','Review','Posted on']) \n",
        "#df\n",
        "({\n",
        "    'name':username,\n",
        "    'rating':starrating,\n",
        "    'title':reviewtitle,\n",
        "    'review':reviewtext,\n",
        "    'time':reviewtime,\n",
        "        columns=['name','rating','title','review','time']\n",
        " \n",
        "})\n",
        "print (reviews)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b711e9ac0da2>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    columns=['name','rating','title','review','time']\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_rx30ogrJP"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxjGsjfvgrJX",
        "outputId": "a6393fa8-4e4e-44ae-82ba-ba3a98a9b7e0"
      },
      "source": [
        "# importing libraries\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "#creating empty lists\n",
        "title=[]\n",
        "venue=[]\n",
        "year=[]\n",
        "authors=[]\n",
        "abstract=[]\n",
        "zipped=[]\n",
        "#for loop for covering increment in page numbers\n",
        "for number in range(0,100,10):\n",
        "    url = \"https://academic.microsoft.com/search?q=football&f=&orderBy=0&skip=\"+str(number)+\"&take=10\"\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text,'html.parser')\n",
        "    #finding attributes\n",
        "    title=soup.find_all('span',class_='au-target')\n",
        "    venue=soup.find_all('span',class_=\"name au-target\")\n",
        "    year=soup.find_all('span',class_=\"year au-target\")\n",
        "    authors=soup.find_all('div',class_='authors')\n",
        "    abstract=soup.find_all('span',class_='au-target')\n",
        "    #using zip function to form lists\n",
        "    zipped == zip(title,venue,year,authors,abstract)\n",
        "#creating dataframe\n",
        "df=pd.DataFrame(list(zipped), columns =['Title', 'Venue','Year','Authors','Abstract'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Venue</th>\n",
              "      <th>Year</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Title, Venue, Year, Authors, Abstract]\n",
              "Index: []"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDBClHTKgrJb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYljWIx3grJd"
      },
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}